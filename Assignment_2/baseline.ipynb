{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmmaBenedetti/DeepLearningProjects/blob/main/baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EO36ZyhT881_"
      },
      "source": [
        "## Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPD5DybyiFVp",
        "outputId": "0d395be2-7a99-401d-8d95-513223c3c859"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QwnJzt4iGbc",
        "outputId": "d4925777-b1ef-40ca-c751-3474de674a71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/DL_assignment2\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/DL_assignment2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lS7SYEju882B"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import glob\n",
        "\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vJA1CXO882E"
      },
      "source": [
        "### Fix random seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jyVna8x6882G"
      },
      "outputs": [],
      "source": [
        "seed = 0\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic=True "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-tFceyg882H"
      },
      "source": [
        "### Import meta info (tokens, number of users )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sYsjPT2F882I"
      },
      "outputs": [],
      "source": [
        "meta = json.load(open('./meta.json', 'r'))\n",
        "tokens = meta['tokens']\n",
        "num_token = len(tokens)\n",
        "num_user = meta['num_user']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR23kGyS882K",
        "outputId": "836328d9-5652-46d5-e0d2-e627e699e4f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In dataset, there are 13369 number of tokens (words) and these tweets are from 8 users\n"
          ]
        }
      ],
      "source": [
        "print('In dataset, there are {} number of tokens (words) and these tweets are from {} users'.format(num_token, num_user))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj9YeZek882L"
      },
      "source": [
        "### Load train and validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tcWRLwhb882M"
      },
      "outputs": [],
      "source": [
        "train_data = json.load(open('./train.json', 'r'))\n",
        "valid_data = json.load(open('./valid.json', 'r'))\n",
        "s_idx = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_y0sNGO882O",
        "outputId": "12a533c2-0a12-4869-8dc6-fc5344b9c253"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6400 tweets in train dataset, 356 tweets in valid dataset.\n",
            "Each json file is a list of dictionary, and each dictionary has information of tweets\n",
            "[TWEET INFO]: user id, sentence, processed token id.\n",
            "\n",
            "Sample train data:  {'user_id': 0, 'sentence': 'i recently met lakeisha crum the first in her family to go to college loved her story', 'token_id': [5721, 9659, 7459, 6629, 2686, 11853, 4447, 5870, 5460, 4236, 12017, 4981, 12017, 2197, 7047, 5460, 11310]}\n",
            "\n",
            "Note that: tokens.index(word) = token_id\n",
            "\n",
            "Example:\n",
            "[5721, 9659, 7459, 6629, 2686, 11853, 4447, 5870, 5460, 4236, 12017, 4981, 12017, 2197, 7047, 5460, 11310]\n",
            "[5721, 9659, 7459, 6629, 2686, 11853, 4447, 5870, 5460, 4236, 12017, 4981, 12017, 2197, 7047, 5460, 11310]\n"
          ]
        }
      ],
      "source": [
        "print('{} tweets in train dataset, {} tweets in valid dataset.'.format(len(train_data), len(valid_data)))\n",
        "print('Each json file is a list of dictionary, and each dictionary has information of tweets')\n",
        "print('[TWEET INFO]: user id, sentence, processed token id.')\n",
        "print()\n",
        "\n",
        "print('Sample train data: ', train_data[s_idx])\n",
        "print()\n",
        "print('Note that: tokens.index(word) = token_id')\n",
        "print()\n",
        "print('Example:')\n",
        "print(train_data[0]['token_id'])\n",
        "print([tokens.index(w) for w in train_data[s_idx]['sentence'].split()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA4FvjGp882Q"
      },
      "source": [
        "### Define Dataset and DataLoader\n",
        "- Note that below code for dataset and dataloader only supports `batch_size = 1`.\n",
        "- Try to find out a way to batchfy the data.\n",
        "- Even if you batchfy the data, put the `token_id` information into `sample['token_id']`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "LKZS6w18882R",
        "outputId": "54c6eed1-1c6f-4fb6-d80f-6178dc652c84"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"class tweetDataset(Dataset):\\n    def __init__(self, data):\\n        self.data = data\\n        \\n    def __len__(self):\\n        return len(self.data)\\n    \\n    def __getitem__(self, idx):\\n        sample = self.data[idx]\\n        sample['token_id'] = torch.Tensor(sample['token_id'])\\n        return sample\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "'''class tweetDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        sample['token_id'] = torch.Tensor(sample['token_id'])\n",
        "        return sample'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKw_vTRK882S",
        "outputId": "94eab5df-b334-4dba-c888-4d6b69681152"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_len_train: 60\n",
            "max_len_val: 47\n",
            "Maximal token length: 60\n"
          ]
        }
      ],
      "source": [
        "# find maximal token length for training and validation dataset\n",
        "max_len_train = 0\n",
        "for diz in train_data:\n",
        "            max_len_train = max(max_len_train, len(diz['token_id']))\n",
        "\n",
        "max_len_val = 0\n",
        "for diz in valid_data:\n",
        "            max_len_val = max(max_len_val, len(diz['token_id']))\n",
        "\n",
        "\n",
        "print(f\"max_len_train: {max_len_train}\\nmax_len_val: {max_len_val}\")\n",
        "max_len = max(max_len_train, max_len_val)\n",
        "print(f\"Maximal token length: {max_len}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Fh7VnTie882U"
      },
      "outputs": [],
      "source": [
        "class tweetDataset(Dataset):\n",
        "    def __init__(self, data, max_len, num_token):\n",
        "        self.data = data\n",
        "        self.max_len = max_len\n",
        "        self.num_token = num_token\n",
        "\n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "\n",
        "        #curr_user = sample['user_id']\n",
        "        curr_token = sample['token_id']\n",
        "        # padding\n",
        "        padding_ls = [self.num_token for _ in range(self.max_len)]\n",
        "        padding_ls[:len(curr_token)] = curr_token\n",
        "\n",
        "\n",
        "        sample['token_id'] = torch.LongTensor(padding_ls[:-1])\n",
        "        sample['output'] = torch.LongTensor(padding_ls[1:])\n",
        "        #sample['user_id'] = torch.Tensor(curr_user)\n",
        "\n",
        "        \n",
        "        '''sample = self.data[idx]\n",
        "        sample['token_id'] = torch.Tensor(sample['token_id'])'''\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OeuoFLD-882X"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "train_dataset = tweetDataset(train_data, max_len, num_token)\n",
        "valid_dataset = tweetDataset(valid_data, max_len,num_token)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhyBfRre882Y",
        "outputId": "518a933a-9589-4b6e-af4b-b33a9bb72909"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 59])\n"
          ]
        }
      ],
      "source": [
        "sample = next(iter(train_dataloader))\n",
        "print(sample['token_id'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0J7QGZa882Y"
      },
      "source": [
        "### Sample datapoint information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSk8VwiF882a",
        "outputId": "7f99918e-240e-4962-a5bd-fd80fc3e1150"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample from train dataloader: \n",
            "USER ID:  tensor([5, 0, 7, 6, 0, 2, 2, 6, 3, 2, 2, 0, 7, 0, 1, 2, 4, 7, 3, 2, 4, 7, 3, 7,\n",
            "        4, 2, 3, 6, 3, 2, 5, 6, 2, 0, 1, 3, 6, 0, 2, 4, 2, 6, 2, 1, 0, 6, 3, 5,\n",
            "        3, 6, 2, 2, 6, 6, 4, 2, 0, 4, 2, 4, 2, 0, 5, 7])\n",
            "TOKEN ID:  tensor([[12949,  5721, 11659,  ..., 13369, 13369, 13369],\n",
            "        [ 6191,  8326,  8241,  ..., 13369, 13369, 13369],\n",
            "        [ 6679,  8012, 10740,  ..., 13369, 13369, 13369],\n",
            "        ...,\n",
            "        [11908,  5109,  1935,  ..., 13369, 13369, 13369],\n",
            "        [ 6983,  1382, 11564,  ..., 13369, 13369, 13369],\n",
            "        [ 5300,  1248, 12017,  ..., 13369, 13369, 13369]])\n",
            "TOKEN ID shape should be BATCH by LENGTH:  torch.Size([64, 59])\n"
          ]
        }
      ],
      "source": [
        "sample = next(iter(train_dataloader))\n",
        "\n",
        "print('Sample from train dataloader: ')\n",
        "print('USER ID: ', sample['user_id'])\n",
        "print('TOKEN ID: ', sample['token_id'])\n",
        "print('TOKEN ID shape should be BATCH by LENGTH: ', sample['token_id'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUoneVlE882b"
      },
      "source": [
        "### Define model based on LSTM\n",
        "- Note that below code for model only supports `batch_size = 1`.\n",
        "- Try to find out a way to use mini-batch.\n",
        "\n",
        "```diff\n",
        "- You must make your class name as \"Model\", as below.\n",
        "- You must make your model work with the input of sample['token_id']\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj8ONdpj882d"
      },
      "source": [
        "#### Other models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-I70Vfac882e",
        "outputId": "1dc67cab-95ba-4472-bd9e-5bf437408d76"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nclass Model(nn.Module):\\n    def __init__(self, num_token, num_user, embed_dim, rnn_dim, num_layers):\\n        super(Model, self).__init__()\\n        self.num_token = num_token\\n        self.num_user = num_user\\n        self.embed_dim = embed_dim\\n        self.rnn_dim = rnn_dim\\n        self.num_layers = num_layers\\n        \\n        self.embedding = nn.Embedding(num_token, embed_dim)\\n        self.rnn = nn.LSTM(embed_dim, rnn_dim, num_layers=num_layers, batch_first=True)\\n        self.out_linear = nn.Linear(rnn_dim, num_user)\\n        \\n    def forward(self, token_id):\\n        embed = self.embedding(token_id)\\n        out, _ = self.rnn(embed)\\n        return self.out_linear(out[:, -1])\\n        '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "### OG MODEL\n",
        "## funziona solo con batch_size = 1\n",
        "'''\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, num_token, num_user, embed_dim, rnn_dim, num_layers):\n",
        "        super(Model, self).__init__()\n",
        "        self.num_token = num_token\n",
        "        self.num_user = num_user\n",
        "        self.embed_dim = embed_dim\n",
        "        self.rnn_dim = rnn_dim\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(num_token, embed_dim)\n",
        "        self.rnn = nn.LSTM(embed_dim, rnn_dim, num_layers=num_layers, batch_first=True)\n",
        "        self.out_linear = nn.Linear(rnn_dim, num_user)\n",
        "        \n",
        "    def forward(self, token_id):\n",
        "        embed = self.embedding(token_id)\n",
        "        out, _ = self.rnn(embed)\n",
        "        return self.out_linear(out[:, -1])\n",
        "        '''\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3sqAUWC882f"
      },
      "source": [
        "#### YOUR MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0pCSiFXE882g"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, num_token, num_user, embed_dim, rnn_dim, num_layers):\n",
        "        super(Model, self).__init__()\n",
        "        self.num_token = num_token +1    # added 1 unit to num_token because we added <EOS>\n",
        "        self.num_user = num_user\n",
        "        self.embed_dim = embed_dim\n",
        "        self.rnn_dim = rnn_dim\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(self.num_token, self.embed_dim)\n",
        "        self.rnn = nn.GRU( input_size = self.embed_dim,\n",
        "                            hidden_size = self.rnn_dim,\n",
        "                            num_layers=self.num_layers,\n",
        "                            #nonlinearity = 'relu',\n",
        "                            batch_first=True)\n",
        "\n",
        "        self.out_linear = nn.Linear(self.rnn_dim, self.num_user)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "    def forward(self, token_id):\n",
        "        embed = self.embedding(token_id)\n",
        "        embed = self.dropout(embed)\n",
        "        out, _ = self.rnn(embed)\n",
        "        return self.out_linear(out[:, -1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_v5LJIR882h"
      },
      "source": [
        "### Make an instance of model and define optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSNCCWgB882h",
        "outputId": "2b735daf-43be-4e10-e447-d0fde72876b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cpu\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"device: {device}\")\n",
        "\n",
        "num_layers = 2\n",
        "model = Model(num_token, num_user, embed_dim=512, rnn_dim=1024, num_layers=num_layers).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-9)\n",
        "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "J6vQdz6ODaUt"
      },
      "outputs": [],
      "source": [
        "# lr=1e-3 is TOO HIGH FOR GRU, accuracy will keep at 30.3%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8L5ilOm882i",
        "outputId": "461e03b8-c403-471f-8b01-3210620e25b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(\n",
            "  (embedding): Embedding(13370, 512)\n",
            "  (rnn): GRU(512, 1024, num_layers=2, batch_first=True)\n",
            "  (out_linear): Linear(in_features=1024, out_features=8, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lduvx4NT882j"
      },
      "source": [
        "### Number of parameter information\n",
        "```diff\n",
        "- The number of parameters should not exceed 20,000,000 !!\n",
        "- DO NOT USE TRANSFORMER-BASED MODELS!!\n",
        "- Transformer-based models will not be accepted as a submission.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alroG2vy882k",
        "outputId": "0cc8a6d5-e2c8-4992-fbe5-8be115c1a2a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 17875976\n",
            "You can use this model\n"
          ]
        }
      ],
      "source": [
        "num_param = sum(p.numel() for p in model.parameters())\n",
        "print(f'Number of parameters: {num_param}')\n",
        "#print('[NOTE] Number of parameters SHOULD NOT exceed 20,000,000 (20 million).')\n",
        "\n",
        "assert num_param < 2*10e7\n",
        "print('You can use this model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0I-9Y2o882l"
      },
      "source": [
        "### Test the model\n",
        "```diff\n",
        "- Test the model if it generates proper output, which shape is B by num_user\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kH4baIPr882m",
        "outputId": "83f389a7-0622-4620-8137-e4bc70cc1a38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 59])\n",
            "Prediction shape would be BATCH X NUM_USER(OUTPUT) :  torch.Size([64, 8])\n"
          ]
        }
      ],
      "source": [
        "print(sample['token_id'].long().size())\n",
        "#pred = model(sample['token_id'].long().to(device))\n",
        "pred = model(sample['token_id'])\n",
        "\n",
        "assert pred.shape == (batch_size,num_user)\n",
        "print('Prediction shape would be BATCH X NUM_USER(OUTPUT) : ', pred.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD8lveSq882m"
      },
      "source": [
        "### Run training for 100 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159,
          "referenced_widgets": [
            "f8e8c8e2ddd74462b19471f2aca090e2",
            "267cf53ab7b148a28ca57501ec1482f6",
            "e23e98ed014545c9b0ed4c29903dd3c9",
            "b69fab7f74e74c9c803c01bc5dc1d9d6",
            "988b9ce29487421b8eccdc6e7562d83a",
            "a12da6dce3d94207ad0aa52e2aed9739",
            "fc77925893e64762a0d10fefc4843b76",
            "3742f01a424f404daea83bb6cdad7989",
            "cffdfd7929504bf4a17dadaac08c738e",
            "893afbd061eb49faa2853b4c2aa737cc",
            "f720e3cba6b840d588bd600b3bb7ebcc"
          ]
        },
        "id": "IgC1p8xW882n",
        "outputId": "5993bf31-c6ee-45e5-b87f-cb11ba3667be"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8e8c8e2ddd74462b19471f2aca090e2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EPOCH 1] BEST VALID ACCURACY UPDATED: 0.486\n",
            "[EPOCH 2] BEST VALID ACCURACY UPDATED: 0.649\n",
            "[EPOCH 3] BEST VALID ACCURACY UPDATED: 0.666\n",
            "[EPOCH 4] BEST VALID ACCURACY UPDATED: 0.733\n",
            "[EPOCH 5] BEST VALID ACCURACY UPDATED: 0.747\n",
            "[EPOCH 24] BEST VALID ACCURACY UPDATED: 0.761\n"
          ]
        }
      ],
      "source": [
        "criteria = nn.CrossEntropyLoss()\n",
        "avg_loss = 0.0\n",
        "best_valid_accu = 0.0\n",
        "best_epoch = -1\n",
        "best_model = None\n",
        "num_epoch = 100\n",
        "\n",
        "\n",
        "for epoch in tqdm(range(num_epoch)):\n",
        "    # start training\n",
        "    for sample in train_dataloader:\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred = model(sample['token_id'].long().to(device))\n",
        "\n",
        "        loss = criteria(pred, sample['user_id'].long().to(device))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # ADDED SCHEDULER\n",
        "        #scheduler.step()\n",
        "\n",
        "        avg_loss += loss.item() / len(train_dataloader)\n",
        "\n",
        "    # start validation\n",
        "    correct_cnt = 0.0\n",
        "    data_cnt = 0.0\n",
        "    for sample in valid_dataloader:\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = model(sample['token_id'].long().to(device))\n",
        "\n",
        "        pred_user_id = torch.argmax(pred, dim=-1)\n",
        "\n",
        "        accu = pred_user_id.detach().cpu() == sample['user_id']\n",
        "\n",
        "        correct_cnt += torch.sum(accu)\n",
        "        data_cnt += sample['token_id'].shape[0]\n",
        "\n",
        "    # calculate best valid accuracy, and save the best model. \n",
        "    curr_valid_accu = (correct_cnt / data_cnt).item()\n",
        "\n",
        "    best_valid_accu = max(best_valid_accu, curr_valid_accu)\n",
        "    if best_valid_accu == curr_valid_accu:\n",
        "        best_epoch = epoch\n",
        "        best_model = copy.deepcopy(model)\n",
        "        torch.save(best_model.state_dict(), f'best_baseline_GRU_{num_epoch}epochs_{num_layers}layers_{batch_size}batch.pth')\n",
        "        print('[EPOCH {}] BEST VALID ACCURACY UPDATED: {}'.format(epoch+1, round(best_valid_accu,3)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6XvYFNa882p"
      },
      "outputs": [],
      "source": [
        "print('FINISHED TRAINING : BEST MODEL AT EPOCH {} WITH ACCURACY {}'.format(best_epoch+1, best_valid_accu))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "baseline_copy.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "kernelspec": {
      "display_name": "py37",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f8e8c8e2ddd74462b19471f2aca090e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_267cf53ab7b148a28ca57501ec1482f6",
              "IPY_MODEL_e23e98ed014545c9b0ed4c29903dd3c9",
              "IPY_MODEL_b69fab7f74e74c9c803c01bc5dc1d9d6"
            ],
            "layout": "IPY_MODEL_988b9ce29487421b8eccdc6e7562d83a"
          }
        },
        "267cf53ab7b148a28ca57501ec1482f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a12da6dce3d94207ad0aa52e2aed9739",
            "placeholder": "​",
            "style": "IPY_MODEL_fc77925893e64762a0d10fefc4843b76",
            "value": " 28%"
          }
        },
        "e23e98ed014545c9b0ed4c29903dd3c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3742f01a424f404daea83bb6cdad7989",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cffdfd7929504bf4a17dadaac08c738e",
            "value": 28
          }
        },
        "b69fab7f74e74c9c803c01bc5dc1d9d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_893afbd061eb49faa2853b4c2aa737cc",
            "placeholder": "​",
            "style": "IPY_MODEL_f720e3cba6b840d588bd600b3bb7ebcc",
            "value": " 28/100 [4:21:46&lt;11:13:10, 560.98s/it]"
          }
        },
        "988b9ce29487421b8eccdc6e7562d83a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a12da6dce3d94207ad0aa52e2aed9739": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc77925893e64762a0d10fefc4843b76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3742f01a424f404daea83bb6cdad7989": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cffdfd7929504bf4a17dadaac08c738e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "893afbd061eb49faa2853b4c2aa737cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f720e3cba6b840d588bd600b3bb7ebcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
